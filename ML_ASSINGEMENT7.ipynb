{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the definition of a target function? In the sense of a real-life example, express the target\n",
    "function. How is a target function's fitness assessed?\n",
    "\n",
    "ANS-The target function is essentially the formula that an algorithm feeds data to in order to calculate predictions\n",
    "As in algebra, it is common when training AI to find the variable from the solution, working in reverse. \n",
    "Analyzing the massive amounts of data related to its given problem, an AI derives understanding of previously unspecified rules by detecting consistencies in the data. The observations of inherent rules about how the studied subject operates inform the AI on how to process future data that does not include an output by applying this previously unknown function.\n",
    "As in algebra, it is common when training AI to find the variable from the solution, working in reverse. The function as defined by f is applied to the input (I) to produce the output (I), Therefore O= f(I).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are predictive models, and how do they work? What are descriptive types, and how do you\n",
    "use them? Examples of both types of models should be provided. Distinguish between these two\n",
    "forms of models.\n",
    "\n",
    "ANS-Predictive modeling is a statistical technique that uses historical or real-time data to predict future outcomes or events12345. It analyzes patterns and trends within specific conditions to determine the most likely outcome1. Predictive modeling is a crucial component of predictive analytics, which uses current and historical data to forecast activity, behavior, and trends45. Businesses often use predictive modeling to forecast sales, understand customer behavior, and mitigate market risks2. Predictive modeling works by analyzing current and historical data and projecting . \n",
    "\n",
    "1. Regression\n",
    "Linear regression, polynomial regression, and logistic regression.\n",
    "2. Neural network\n",
    "Multilayer perceptron (MLP), convolutional neural networks (CNN), recurrent neural networks (RNN), backpropagation, feedforward, autoencoder, and Generative Adversarial Networks (GAN)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Describe the method of assessing a classification model'\n",
    "s efficiency in detail. Describe the various\n",
    "measurement parameters.\n",
    "\n",
    "ANS-I will cover 4 metrics commonly used to evaluate classification models. These include accuracy, precision, recall, and the F1 score.\n",
    "True Positives (TP) are instances where the classifier predicts a 1, and the true value is 1\n",
    "False Positives (FP) are instances where the classifier predicts a 1, and the true value is 0\n",
    "False Negatives (FN) are instances where the classifier predicts a 0, and the true value is 1  \n",
    "True Negatives (TN) are instances and the classifier predicts a 0, where the true value is 0\n",
    "\n",
    "Accuracy\n",
    " Accuracy measures the fraction of correctly classified samples. The following equation defines this value:\n",
    "\n",
    "\n",
    "\n",
    "Accuracy= \n",
    "TP+FP+TN+FN\n",
    "TP+TN\n",
    "​\n",
    "     (1)\n",
    "\n",
    "Precision\n",
    "Precision measures the ability of the classifier to correctly label positive values. The following equation defines this value:\n",
    "\n",
    "\n",
    "Precision= \n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    "     (2)\n",
    "\n",
    "Recall\n",
    "Recall measures the ability of the model to find all positive values. The following equation defines this value:\n",
    "\n",
    "\n",
    "Recall= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    "     (3)\n",
    "\n",
    "F1 Score\n",
    "The F1 score is a weighted average of the precision and recall metrics. The following equation defines this value:\n",
    "\n",
    "\n",
    "\n",
    "F1= \n",
    "Precision+Recall\n",
    "2×Precision×Recall\n",
    "​\n",
    "     (4)\n",
    "\n",
    "Note that for all of the metrics above, the best possible outcome is 1.0. The worst possible result is a 0.0. Next let’s cover the two plotting techniques I mentioned earlier.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. In the sense of machine learning models, what is underfitting? What is the most common\n",
    "reason for underfitting?\n",
    "\n",
    "\n",
    "ANS-\n",
    "Underfitting: A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data. (It’s just like trying to fit undersized pants!) Underfitting destroys the accuracy of our machine learning model.\n",
    "\n",
    "Reasons for Underfitting:\n",
    "\n",
    "High bias and low variance \n",
    "The size of the training dataset used is not enough.\n",
    "The model is too simple.\n",
    "Training data is not cleaned and also contains noise in it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. What does it mean to overfit? When is it going to happen?\n",
    "\n",
    "ANS-Overfitting: A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models\n",
    "\n",
    "Reasons for Overfitting are as follows:\n",
    " High variance and low bias \n",
    "The model is too complex\n",
    "The size of the training data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. In the sense of model fitting, explain the bias-variance trade-off.\n",
    "\n",
    "ANS-Bias\n",
    "Bias is simply defined as the inability of the model because of that there is some difference or error occurring between the model’s predicted value and the actual value. These differences between actual or expected values and the predicted values are known as error or bias error or error due to bias. \n",
    "\n",
    "Low Bias: Low bias value means fewer assumptions are taken to build the target function. In this case, the model will closely match the training dataset.\n",
    "High Bias: High bias value means more assumptions are taken to build the target function. In this case, the model will not match the training dataset closely. \n",
    "\n",
    "Variance\n",
    "Variance is the measure of spread in data from its mean position. In machine learning variance is the amount by which the performance of a predictive model changes when it is trained on different subsets of the training data. More specifically, variance is the variability of the model that how much it is sensitive to another subset of the training dataset. i.e how much it can adjust on the new subset of the training dataset.\n",
    "\n",
    "Low variance: Low variance means that the model is less sensitive to changes in the training data and can produce consistent estimates of the target function with different subsets of data from the same distribution. This is the case of underfitting when the model fails to generalize on both training and test data.\n",
    "High variance: High variance means that the model is very sensitive to changes in the training data and can result in significant changes in the estimate of the target function when trained on different subsets of data from the same distribution. This is the case of overfitting when the model performs well on the training data but poorly on new, unseen test data. It fits the training data too closely that it fails on the new training dataset.\n",
    "Ways to Reduce the redu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Is it possible to boost the efficiency of a learning model? If so, please clarify how.\n",
    "\n",
    "\n",
    "\n",
    "ANS-Without good performance, machine learning (ML) models won’t provide much value in real life. We’ll introduce some common strategies to improve model performance including selecting the best algorithm, tuning model settings, and adding new features (aka feature engineering).\n",
    "\n",
    "Compare multiple algorithms\n",
    "\n",
    "Hyperparameter tuning\n",
    "\n",
    "Trade precision with recall\n",
    "\n",
    "Feature engineering\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model?\n",
    "\n",
    "ANS-In unsupervised learning, there is no labeled output to evaluate the performance of the model, so it is not possible to use traditional accuracy metrics. Instead, success of unsupervised learning models is evaluated by other measures such as clustering evaluation metrics, reconstruction error, visualization, and interpretability1.\n",
    "\n",
    "Clustering evaluation metrics include measures such as silhouette score and elbow method1. Reconstruction error is the difference between the input data and the output data from the model2. Visualization can be used to understand the structure of the data and how it is being clustered3. Interpretability refers to how well the model can be understood by humans1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer.\n",
    "\n",
    "ANS-Classification models are used for categorical data and regression models are used for numerical data. The reason is that classification models are designed to predict the probability of an object belonging to a particular class or category, while regression models are designed to predict a continuous value123. Therefore, it is not possible to use a classification model for numerical data or a regression model for categorical data4."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?\n",
    "\n",
    "ANS-There are several algorithms for predicting numerical values. Some of the most common ones include Linear Regression, Decision Trees, Neural Networks, and K-Nearest Neighbors1.\n",
    "\n",
    "Linear regression attempts to fit a straight hyperplane to your dataset that is closest to all data points1. Decision Trees are a tree-like model of decisions and their possible consequences2. Neural Networks are a set of algorithms that try to recognize patterns in data3. K-Nearest Neighbors is a non-parametric algorithm that stores all available cases and classifies new cases based on a similarity measure\n",
    "\n",
    "Categorical predictive modeling is a type of predictive modeling that deals with categorical data. It is used to predict the probability of an event occurring based on the input variables. On the other hand, predictive modeling is a process that uses data mining and machine learning techniques to create models that can predict future outcomes1. Predictive modeling can be used for both categorical and continuous data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. The following data were collected when using a classification model to predict the malignancy of a group of patients' tumors:\n",
    "         i. Accurate estimates – 15 cancerous, 75 benign\n",
    "         ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "                Determine the model's error rate, Kappa value, sensitivity, precision, and F-measure\n",
    "\n",
    "\n",
    "ANS-\n",
    "TP=15,\n",
    "TN=75,\n",
    "FP=3,\n",
    "FN=7\n",
    "  \n",
    "  ERROR RATE=(FP+FN)/(TP+TN+FP+FN)\n",
    "     >>>       =(3+7)/100\n",
    "        >>   = 0.1\n",
    "\n",
    "KAPPA VALUE =(Po-Pe)/(1-PE)\n",
    "\n",
    "Po=15+75= 90%=0.9\n",
    "\n",
    "Pe=(18*22)+(78*82)/100*100 = 0.6792\n",
    "\n",
    "=0.2208/0.3204\n",
    "\n",
    "=0.684\n",
    "\n",
    "Sensitivity = TP / (TP + FN)\n",
    "\n",
    "Sensitivity = 15 / (15 + 3) = 0.83\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Precision = 15 / (15 + 7) = 0.68\n",
    "\n",
    "F-measure = 2 * ((Precision * Recall) / (Precision + Recall))\n",
    "\n",
    "F-measure = 2 * ((0.68 * 0.83) / (0.68 + 0.83)) = 0.743."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Make quick notes on:\n",
    "         1. The process of holding out\n",
    "\n",
    "\n",
    "The hold-out method for training a machine learning model is the process of splitting the data into different splits and using one split for training the model and other splits for validating and testing the models. The hold-out method is used for both model evaluation and model selection.\n",
    "The following is the process of using the hold-out method for model evaluation:\n",
    "\n",
    "Split the dataset into two parts (preferably based on a 70-30% split; However, the percentage split will vary)\n",
    "Train the model on the training dataset; While training the model, some fixed set of hyperparameters is selected.\n",
    "Test or evaluate the model on the held-out test dataset\n",
    "Train the final model on the entire dataset to get a model which can generalize better on the unseen or future dataset.\n",
    "\n",
    "The following process represents the hold-out method for model selection:\n",
    "\n",
    "Split the dataset in three parts – Training dataset, validation dataset and test dataset.\n",
    "Train different models using different machine learning algorithms. For example, train the classification model using logistic regression, random forest, XGBoost, etc.\n",
    "For the models trained with different algorithms, tune the hyper-parameters and come up with different models. For each of the algorithms mentioned in step 2, change hyperparameters settings and come with multiple models.\n",
    "Test the performance of each of these models (belonging to each of the algorithms) on the validation dataset.\n",
    "Select the most optimal model out of the models tested on the validation dataset. The most optimal model will have the most optimal hyperparameters settings for a specific algorithm. Going by the above example, let’s say the model trained with XGBoost with the most optimal hyperparameters gets selected.\n",
    "Test the performance of the most optimal model on the test dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Cross-validation by tenfold\n",
    "\n",
    "10 fold cross –validation\n",
    "- The standard way of predicting the error rate of a learning techniquegiven a single, fixed sample of data is to use stratified tenfold cross-validation. The data is dividedrandomly into 10 parts in which the class is represented in approximately the same proportions as inthe full dataset. Each part is held out in turn and the learning scheme trained on the remaining nine-tenths; then its error rate is calculated on the holdout set. When seeking an accurate error estimate, itis standard procedure to repeat the cross-validation process 10 times—that is, 10 times tenfold cross-validation—and average the results. Tenfold cross-validation is the standard way of measuring theerror rate of a learning scheme on a particular dataset; for reliable results, 10 times tenfold cross-validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 3. Adjusting the parameters\n",
    "\n",
    " In machine learning, adjusting parameters is an important step in optimizing the performance of a model. Hyperparameters are parameters that can be fine-tuned and adjusted to increase the accuracy score of a machine learning model1. Machine algorithms such as Random forest, K-Nearest Neighbor and Decision trees have parameters that can be fine-tuned to achieve an optimized model1.\n",
    "\n",
    "There are various tradeoffs in adjusting hyperparameters that affect model training time. If the learning rate configured through hyperparameters is set very low, the model takes too long to converge to an optimal solution. If it is set too high, it may not converge at all2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Define the following terms: \n",
    "         \n",
    "         1. Purity vs. Silhouette width\n",
    "\n",
    "\n",
    "Purity and silhouette width are both clustering quality measures used to evaluate the performance of clustering algorithms. Silhouette width measures how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1, where 1 indicates that the object is well-matched to its own cluster and poorly matched to neighboring clusters. Purity measures the percentage of objects in a cluster that belong to the same class. It ranges from 0 to 1, where 1 indicates that all objects in a cluster belong to the same class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "         2. Boosting vs. Bagging\n",
    "\n",
    "\n",
    "Bagging: It is a homogeneous weak learners’ model that learns from each other independently in parallel and combines them for determining the model average.\n",
    "\n",
    "Boosting: It is also a homogeneous weak learners’ model but works differently from Bagging. In this model, learners learn sequentially and adaptively to improve model predictions of a learning algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       3. The eager learner vs. the lazy learner\n",
    "\n",
    "Lazy and eager learning are two types of machine learning methods that differ in how they generalize from training data1. Eager learning builds a global model from the training data before making predictions, while lazy learning stores the training data or builds local models on demand12. Eager learning has faster query times but may be less accurate or flexible than lazy learning3. Lazy learning can handle large-scale data streams but may be slower or more memory-intensive than eager learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
