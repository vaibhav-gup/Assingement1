{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What exactly is a feature? Give an example to illustrate your point.\n",
    "\n",
    "ANS-In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\n",
    "\n",
    "A numeric feature can be conveniently described by a feature vector. One way to achieve binary classification is using a linear predictor function (related to the perceptron) with a feature vector as input. The method consists of calculating the scalar product between the feature vector and a vector of weights, qualifying those observations whose result exceeds a threshold.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the various circumstances in which feature construction is required?\n",
    "\n",
    "ANS-Feature construction is the process of creating new features from existing ones to improve the performance of machine learning tasks. It is subjective and requires human creativity and intervention. The new features are created by mixing existing features using addition, subtraction, and ratio, and these new features have great flexibility1.\n",
    "\n",
    "Feature construction is required in various circumstances such as when the existing features are not sufficient to represent the underlying patterns in the data, when the existing features are noisy or irrelevant, when the existing features are too complex or too simple, or when the existing features are not interpretable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Describe how nominal variables are encoded.\n",
    "\n",
    "ANS-\n",
    "Nominal variables are encoded with One-Hot-Encoding. Ordinal variables are encoded with Ordinal Encoding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Describe how numeric features are converted to categorical features.\n",
    "\n",
    "ANS-Discretization: It is the process of transforming continuous variables into categorical variables by creating a set of intervals, which are contiguous, that span over the range of the variable’s values. It is also known as “Binning”, where the bin is an analogous name for an interval."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this\n",
    "approach?\n",
    "\n",
    "ANS-The feature selection wrapper approach is a method of feature selection that involves selecting features based on their performance on a machine learning algorithm. The algorithm is trained on different subsets of features, and the performance of each subset is evaluated. The subset that performs the best is selected as the final set of features1.\n",
    "\n",
    "The main advantage of the wrapper approach is that it can select features that are critical for certain algorithms1. However, it can be computationally expensive, requiring repeated training of the machine learning algorithm on various subsets of features1. Moreover, it can be highly sensitive to overfitting, especially if the number of features is large relative to the number of samples1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. When is a feature considered irrelevant? What can be said to quantify it?\n",
    "\n",
    "ANS=A feature is considered irrelevant in machine learning if it is conditionally independent of the class labels or it does not influence the class labels; in these cases, it can be discarded1. Any feature which is irrelevant in the context of a machine learning task is a candidate for rejection when we are selecting a subset of features2.\n",
    "\n",
    "In addition, a feature is strongly relevant if removing it degrades the B.O.C. (Business Objective Criteria). A feature is instead weakly relevant if it is not strongly relevant. A subset of features S exists, such that adding the feature to S improves the B.O.C. A feature is otherwise irrelevant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. When is a function considered redundant? What criteria are used to identify features that could\n",
    "be redundant?\n",
    "\n",
    "ANS-A function is considered redundant if it can be derived from any other function or set of functions1. In machine learning, an attribute (column or feature of data set) is called redundant if it can be derived from any other attribute or set of attributes1.\n",
    "\n",
    "To identify features that could be redundant, one can use correlation analysis. Correlation analysis measures the strength of the relationship between two variables. If two variables are highly correlated, then one of them may be redundant and can be removed from the dataset1. Another approach is to use principal component analysis (PCA), which is a technique used to reduce the dimensionality of a dataset while retaining as much of the variation in the data as possible2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. what are the various distance measurements used to determine feature similarity?\n",
    "\n",
    "ANS-Distance metrics are used in supervised and unsupervised learning to calculate similarity in data points. They improve the performance, whether that’s for classification tasks or clustering. The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. State difference between Euclidean and Manhattan distances?\n",
    "\n",
    "ANS-The Manhattan distance is the shortest path between two points that is parallel to the coordinate axes system. Euclidean distance is the straight line distance between two point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Distinguish between feature transformation and feature selection.\n",
    "\n",
    "ANS-Feature selection is the process of selecting a subset of relevant features for use in model construction. Feature transformation is the process of transforming data to improve the accuracy of the algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Make brief notes on any two of the following:\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "a hybrid approach to extract the important attributes called features of a product or a service or a professional from the textual reviews/ feedbacks has been proposed. The approach makes use of topic modeling concepts and the linguistic knowledge embedded in the text using Natural Language Processing tools. A system has been implemented and tested taking the feedbacks of two different domains: feedback of teachers and feedback of laptops. The system tries to extract all those features (single word and multiple words) for which users have expressed their opinion in the reviews. The syntactic category and the frequency contribute in deciding the importance of a feature and a numerical value between zero and one called weight is generated for each identified feature representing its significance. Results obtained from the proposed system are comparable if extraction from the text is done manually.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Receiver operating characteristic curve\n",
    "\n",
    "The receiver operating characteristic curve is a popular graphical method frequently used in order to study the diagnostic capacity of continuous markers. It represents in a plot true-positive rates against the false-positive ones. Both the practical and theoretical aspects of the receiver operating characteristic curve have been extensively studied. Conventionally, it is assumed that the considered marker has a monotone relationship with the studied characteristic; i.e., the upper (lower) values of the (bio)marker are associated with a higher probability of a positive result. However, there exist real situations where both the lower and the upper values of the marker are associated with higher probability of a positive result. We propose a receiver operating characteristic curve generalization, [Formula: see text], useful in this context. All pairs of possible cut-off points, one for the lower and another one for the upper marker values, are taken into account and the best of them are selected. The natural empirical estimator for the [Formula: see text] curve is considered and its uniform consistency and asymptotic distribution are derived. Finally, two real-world applications are studied."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
